# -*- coding: utf-8 -*-
"""Rice Image Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jyimzHiBVTrkfQ5fXNRg77pL3KOfTF7v

##Downloading the Dataset
"""

import kagglehub

path = kagglehub.dataset_download("muratkokludataset/rice-image-dataset")

print("Path to dataset files:", path)

"""##Import the Libraries"""

!pip install --upgrade kagglehub

import kagglehub

! pip install -q silence_tensorflow
from silence_tensorflow import silence_tensorflow
silence_tensorflow()
import warnings
warnings.filterwarnings('ignore')
import os
import time
import random
import keras
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
from tabulate import tabulate
from termcolor import colored
import matplotlib.pyplot as plt
from IPython.display import display
from keras.models import Sequential
from keras import models, layers, optimizers
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import confusion_matrix, classification_report
from keras.src.legacy.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
sns.set_style('whitegrid')

"""##Dataset Inspection"""

dir1 = '/kaggle/input/rice-image-dataset/Rice_Image_Dataset'

# This lists the subdirectories within the dataset's Rice_Image_Dataset folder.
detection = [class_name for class_name in os.listdir(dir1) if os.path.isdir(os.path.join(dir1, class_name))]
print(detection)

"""##Ploting and Count Images(5 categories of rice)"""

for class_name in detection:
    # Here I had Prepared class path
    class_path = os.path.join(dir1, class_name)
    # Then Listing images
    images = os.listdir(class_path)
    random_images = random.choices(images, k=5)

    fig, ax = plt.subplots(1, 5, figsize=(15, 3))
    plt.suptitle(f'Samples from Class: {class_name}', fontsize=18, fontweight='bold', color='#1A3636')

    for i in range(5):
        # Loaded and displayed the image
        img_path = os.path.join(class_path, random_images[i])
        img = keras.utils.load_img(img_path)
        ax[i].imshow(img)
        ax[i].axis('off')  # Removed axis

        # Its optioonal here we can add image title
        ax[i].set_title(f'Image {i + 1}', fontsize=10)

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

# Created a list of tuples that contain image paths and their class labels
image_data = []  # Initialize an empty list for storing image paths and labels

# Iterated through each class in the detection
for class_name in detection:
    class_path = os.path.join(dir1, class_name)  # Create the path for the class
    files = os.listdir(class_path)  # List all files in the class folder

    # Looping through all files and add their paths and class labels to the image_data list
    for file in files:
        file_path = os.path.join(class_path, file)  # Created the full file path
        image_data.append((file_path, class_name))  # Appended tuple to the list

# Converted the list to a pandas DataFrame
df = pd.DataFrame(image_data, columns=['path', 'label'])

# Shuffle the DataFrame rows for randomness
df = df.sample(frac=1).reset_index(drop=True)

display(df.head())

total_samples = len(df)
print(colored(f'Total number of samples: {total_samples}', 'green', attrs=['underline']))

training_data = df[:60000]
validation_data = df[60000:67500]
test_data = df[67500:]

training_data.reset_index(inplace=True, drop=True)
validation_data.reset_index(inplace=True, drop=True)
test_data.reset_index(inplace=True, drop=True)

print(colored(f'Number of samples in Training df: {len(training_data)}', attrs=['bold']))
print(colored(f'Number of samples in Validation df: {len(validation_data)}', attrs=['bold']))
print(colored(f'Number of samples in Test df: {len(test_data)}', attrs=['bold']))

"""##Image Preprocessing"""

# Setting image dimensions and batch size
IMAGE_SIZE = (224, 224)    #This sets the target size for images.
BATCH_SIZE = 16            #Defined samples per batch

# Initialize the ImageDataGenerator for image normalization
data_gen = ImageDataGenerator(rescale=1.0/255)

def create_data_generator(dataframe, is_training=False):
    return data_gen.flow_from_dataframe(
        dataframe,
        x_col='path',
        y_col='label',
        target_size=IMAGE_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=is_training)

# Here I had  created generators for training, validation and test datasets
train_generator = create_data_generator(training_data, is_training=True)
valid_generator = create_data_generator(validation_data, is_training=False)
test_generator = create_data_generator(test_data, is_training=False)

# input shape of the model
input_shape = (224, 224, 3)

# Building the custom CNN model
model = Sequential()

# Convolutional and MaxPooling layers
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))  # Convolutional layer with 32 filters of size 3x3
model.add(BatchNormalization())  # Normalizes the output of the previous layer
model.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling layer to reduce dimensions

model.add(Conv2D(64, (3, 3), activation='relu'))  # Another convolutional layer with 64 filters
model.add(BatchNormalization())  # Normalize again
model.add(MaxPooling2D(pool_size=(2, 2)))  # Another max pooling layer

# Flatten the output and add Dense layers
model.add(Flatten())  # Converts the 2D matrix to a 1D vector
model.add(Dense(512, activation='relu'))  # Fully connected layer with 512 units
model.add(Dropout(0.5))  # Dropout layer for regularization
model.add(Dense(64, activation='relu'))  # Another dense layer with 64 units
model.add(Dropout(0.5))  # Another dropout layer
model.add(Dense(len(training_data['label'].unique()), activation='softmax'))  # Output layer for the number of classes

# Compiled the model
model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(learning_rate=0.01), metrics=['accuracy'])

model.summary()

print("Custom CNN model is built and ready for training!")

"""##Saving the best model during training"""

checkpoint_callback = keras.callbacks.ModelCheckpoint('BestModel.keras', save_best_only=True)

# ReduceLROnPlateau Callback to decrease the learning rate based on the 'monitor' parameter after a specified 'patience' period if no improvement is observed
reduce_learning_rate = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Model Training
numepochs = 10

# Starting the timer
start_time = time.time()

history = model.fit(train_generator, epochs=numepochs, validation_data=test_generator,
                    callbacks=[checkpoint_callback, reduce_learning_rate])

# Stopping the timer
end_time = time.time()

# Here we get total training time
training_time = end_time - start_time
print(f"\nTotal training time: {training_time:.2f} seconds")

result_df = pd.DataFrame(history.history)
print(tabulate(result_df, headers='keys', tablefmt='psql'))

# Storing range of epochs
x = np.arange(len(result_df))

fig, ax = plt.subplots(1, 3, figsize=(18, 4))
fig.suptitle('Model Performance', fontsize=16)

# ax[0] ---> Accuracy
ax[0].plot(x, result_df.accuracy, label='Train Accuracy', linewidth=2, color='#677D6A')
ax[0].plot(x, result_df.val_accuracy, label='Validation Accuracy', linewidth=2, linestyle='--', color='#D6BD98')
ax[0].set_title('Accuracy', fontsize=14)
ax[0].set_yticks(np.arange(0.95, 1.01, 0.01))
ax[0].set_xticks(np.arange(0, len(result_df), 2))
ax[0].set_xlabel('Epochs', fontsize=12)
ax[0].set_ylabel('Accuracy', fontsize=12)
ax[0].legend()
ax[0].grid(True)

# ax[1] ---> Loss
ax[1].plot(x, result_df.loss, label='Train Loss', linewidth=2, color='#677D6A')
ax[1].plot(x, result_df.val_loss, label='Validation Loss', linewidth=2, linestyle='--', color='#D6BD98')
ax[1].set_title('Loss', fontsize=14)
ax[1].set_xticks(np.arange(0, len(result_df), 2))
ax[1].set_yticks(np.arange(0, 0.085, 0.01))
ax[1].set_xlabel('Epochs', fontsize=12)
ax[1].set_ylabel('Loss', fontsize=12)
ax[1].legend()
ax[1].grid(True)

# ax[2] ---> Learning Rate
ax[2].plot(x, result_df.learning_rate, label='Learning Rate', linewidth=2, marker='o', color='#677D6A')
ax[2].set_title('Learning Rate', fontsize=14)
ax[2].set_xticks(np.arange(0, len(result_df), 2))
ax[2].set_xlabel('Epochs', fontsize=12)
ax[2].set_ylabel('Learning Rate', fontsize=12)
ax[2].legend()
ax[2].grid(True)

plt.tight_layout(rect=[0, 0, 1, 0.96])

plt.show()

# I had reset the test data generator to start from the beginning.
test_generator.reset()

# Getting the next batch of images and their corresponding labels from the test generator.
img, label = next(test_generator)

# Generating predictions for the batch of images using the trained model.
predictions = model.predict(img)

# Extracting the predicted classes from the model's output.
test_pred_classes = np.argmax(predictions, axis=1)

plt.figure(figsize=[16, 16])

# Looping through the first 16 images in the batch to display them along with their labels and predictions.
for i in range(16):
    plt.subplot(4, 4, i + 1)
    plt.imshow(img[i])
    plt.axis('off')  # Hiding the axes for a cleaner look

    # Title with the true label, predicted class, and prediction confidence percentage
    plt.title(f"Label: {detection[np.argmax(label[i])]}\n"
              f"Prediction: {detection[test_pred_classes[i]]} "
              f"({100 * np.max(predictions[i]):.1f}%)",
              fontsize=12, color='#1A3636')

plt.tight_layout()
plt.show()

"""##Detailed Classification Report"""

# Generated predictions on the test set
print("Generating predictions on the test set...")
y_pred = model.predict(test_generator, verbose=1)

# Converted predicted probabilities to class labels
y_pred_classes = np.argmax(y_pred, axis=1)

# Generated a classification report
print("\nClassification Report:")
clf = classification_report(test_generator.classes, y_pred_classes, target_names=detection)

print("="*50)
print("Detailed Classification Report")
print("="*50)
print(clf)
print("="*50)

